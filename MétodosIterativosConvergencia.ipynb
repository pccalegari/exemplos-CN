{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOq6v9eaw9M8Yfl8n+/ykvF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pccalegari/exemplos-CN/blob/main/M%C3%A9todosIterativosConvergencia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Convergência de métodos iterativos**\n",
        "\n",
        "Considere o sistema $A{\\bf x} = {\\bf b}$ e vamos escrever $A=L+D+U$, onde $L$ é a parte triangular inferior de $A$, $D$ é a uma matriz diagonal que conté a diagonal da matriz $A$ e $U$ é a parte triangular superior de $A$.\n",
        "\n",
        "A versão matricial da fórmula de iteração do método de Jacobi é dada por,\n",
        "\n",
        "$${\\bf x}^{(k+1)} = D^{-1}({\\bf b} - L{\\bf x}^{(k)} - U{\\bf x}^{(k)}),$$\n",
        "\n",
        "pois, $A{\\bf x} = {\\bf b} \\Longrightarrow (L+D+U){\\bf x} = {\\bf b} \\Longrightarrow L{\\bf x} + D{\\bf x} + U{\\bf x} = {\\bf b}$. Isolar cada uma das incógnitas em cada uma das equações é equivalente a\n",
        "$$D{\\bf x} = {\\bf b} - L{\\bf x} - U{\\bf x} \\Longrightarrow {\\bf x}^{(k+1)} = D^{-1}({\\bf b}-L{\\bf x}^{(k)}-U{\\bf x}^{(k)}).$$\n",
        "\n",
        "De forma similar, obtemos a versão matricial do método de Gauss-Seidel,\n",
        "\n",
        "$${\\bf x}^{(k+1)} = (L+D)^{-1}({\\bf b} - U{\\bf x}^{(k)}),$$\n",
        "\n",
        "pois, $A{\\bf x} = {\\bf b} \\Longrightarrow (L+D+U){\\bf x} = {\\bf b} \\Longrightarrow L{\\bf x} + D{\\bf x} + U{\\bf x} = {\\bf b} \\Longrightarrow (L+D){\\bf x}^{(k+1)} = {\\bf b} - U{\\bf x}^{(k)}.$\n",
        "\n",
        "A análise de convergência dos dois métodos é feita reescrevendo-os  na forma geral:\n",
        "\n",
        "$${\\bf x}^{(k+1)} = B{\\bf x}^{(k)}+ c,$$\n",
        "onde $B$ é chamada matriz de iteração. No método de Jacobi, temos $B_J=-D^{-1}(L+U)$ e para o método de Gauss-Seidel, $B_{GS} = -(L+D)^{-1}U$. A convergência é estudada por meio da análise dos autovalores da matriz de iteração $B$.\n",
        "\n"
      ],
      "metadata": {
        "id": "1_H5-CIf0NJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Definição:*\n",
        "\n",
        "Dizemos que $\\lambda_i$, $i=1,2,\\ldots,n$ é autovalor da matriz $B$ se $B{\\bf u}=\\lambda_i {\\bf u}$, paa algum vetor não nulo ${\\bf u}$.\n",
        "\n",
        "*Teorema:*\n",
        "\n",
        "O método iterativo ${\\bf x}^{(k+1)}=B{\\bf x}^{(k)}+{\\bf c}$ gera uma sequência convergente se, e somente se, os autovalores de $B$ satisfazem $\\max_{1\\le i\\le n} |\\lambda_i| < 1.$"
      ],
      "metadata": {
        "id": "6qG5DBWP2nug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Critérios de Convergência:*\n",
        "\n",
        "(1) Critério das linhas:\n",
        "\n",
        "Uma condição suficiente para garantir a convergência dos métodos iterativos é que a matriz $A$ seja estritamente diagonal dominante. Ou seja,\n",
        "\n",
        "$$|a_{ii}| > \\sum_{j=1,j\\neq i}^n |a_{ij}|, \\forall i=1,\\ldots, n.$$\n",
        "\n",
        "(2) Critério de Sassenfeld:\n",
        "\n",
        "Vamos definir\n",
        "\n",
        "$$\\beta_1 = \\dfrac{1}{|a_{11}|}\\sum_{j=2}^{n}|a_{1j}|,$$\n",
        "$$\\beta_i=\\dfrac{1}{|a_{ii}|}\\left(\\sum_{j=1}^{i-1}|a_{ij}|\\beta_j + \\sum_{j=i+1}^n|a_{ij}|\\right), \\mbox{para} i=2,\\ldots,n$$\n",
        "e $M=\\max_{1\\le i\\le n}\\beta_i$. Se $M< 1$ então o método iterativo converge.\n",
        "\n",
        "*Exemplo 1:* Verifique se é possível garantir a convergência dos métodos iterativos para sistemas de equações lineares com as matrizes:\n",
        "\n",
        "$\\displaystyle{A_1=\\left(\\begin{array}{ccc}\n",
        "4 & 1 & -2 \\\\\n",
        "-1 & -1 & 5\\\\\n",
        "2 & 3 & 6\\\\\n",
        "\\end{array}\\right)}$ $\\displaystyle{A_2=\\left(\\begin{array}{cccc}\n",
        "2 & 1 & 0 & 0 \\\\\n",
        "-1 & 2 & -1 & 0\\\\\n",
        "0 & -1 & 2 & -1\\\\\n",
        "0 & 0 & -1 & 2\\\\\n",
        "\\end{array}\\right)}$\n",
        "\n",
        "*Exemplo 2:* Verifique se é possível garantir a convergência dos métodos iterativos para o sistema de equações lineares com a matriz:\n",
        "\n",
        "$\\displaystyle{A_3=\\left(\\begin{array}{ccc}\n",
        "8 & 2 & 1 \\\\\n",
        "5 & 4 & 1 \\\\\n",
        "4 & 3 & 3\\\\\n",
        "\\end{array}\\right)}$\n"
      ],
      "metadata": {
        "id": "LMJUkiSt7zWo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwScZTYZde0d"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "A = np.array([[8,2,1],[5,4,1],[4,3,3]])\n",
        "#print(A)\n",
        "\n",
        "D = np.array([[8,0,0],[0,4,0],[0,0,3]])\n",
        "LU = A - D\n",
        "BJ = np.dot(np.linalg.inv(D),-LU)\n",
        "aut = np.linalg.eigvals(BJ)\n",
        "print(abs(aut))\n",
        "\n",
        "U = np.array([[0,2,1],[0,0,1],[0,0,0]])\n",
        "LD = A - U\n",
        "#print(LD)\n",
        "BGS = np.dot(np.linalg.inv(LD),-U)\n",
        "#print(BGS)\n",
        "autgs = np.linalg.eigvals(BGS)\n",
        "print(autgs)\n",
        "\n",
        "amax = abs(autgs[1])\n",
        "print(amax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Métodos SOR**\n",
        "\n",
        "Uma técnica de aceleração de convergência dos métodos iteativos estudados chamada relaxação sucessiva. A aproximação na iteração $(k+1)$ é uma média entre o valor  ${\\bf x}^{(k)}$ e o valor ${\\bf x}^{(k+1)}$ obtido por Gauss-Seidel.\n",
        "\n",
        "$${\\bf x}_{SOR}^{(k+1)}=(1-w){\\bf x}_{SOR}^{(k)}+w{\\bf x}_{GS}^{(k+1)},$$\n",
        "$w$ é chamado parâmetro de relaxação. Se $w=1$ temos o método de Gauss-Seidel. O método só converge se $0 < w < 2$. Se $1< w< 2$ temos os métodos de sobre-relaxação e se $0< w < 1$ temos os métodos de sub-relaxação. Na prática, em geral, melhores resultados são obtidos com a sobre-relaxação. Em alguns casos é possível determinar o parâmetro $w$ ótimo. Por exemplo, se $A$ é uma matriz tridiagonal e definida positiva então $\\displaystyle{w_{ot} = \\dfrac{2}{1+\\sqrt{1-\\rho^2}}}$, sendo $\\rho$ o raio espectral (maior autovalor em valor absoluto) da matriz de iteração do método de Jacobi, $B_{J}$."
      ],
      "metadata": {
        "id": "Hr7UMOlDNvKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Exemplo:*\n",
        "\n",
        "Implemente os métodos iterativos estudados para resolver o sistema:\n",
        "\n",
        "$$\\left\\{\\begin{array}{cccccccccccc}\n",
        "2x_1 & - & x_2 &  & & = & 1\\\\\n",
        "-x_{i-1} & + & 2x_i & - & x_{i+1} & = & 0 & \\mbox{para } i=2,\\ldots,n-1\\\\\n",
        "& - & x_{n-1} & + & 2x_n & = & 1\n",
        "\\end{array}\\right .$$\n",
        "\n"
      ],
      "metadata": {
        "id": "FPvrhfzK51dj"
      }
    }
  ]
}