{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdaI5LwJYQitBdDgp5IX1Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pccalegari/exemplos-CN/blob/main/Unidade3_SistemasLineares_M%C3%A9todosIterativos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sistemas Lineares - Métodos iterativos**\n",
        "\n",
        "*Objetivo:* Dado $A{\\bf x} = {\\bf b}$, construir uma sequência de aproximações ${\\bf x}^{(k)}$ para a solução ${\\bf x}^* = A^{-1}{\\bf b}$, tal que ${\\bf x}^{(k)}\\longrightarrow {\\bf x}^*$ quando $k\\longrightarrow \\infty$.\n",
        "\n",
        "Lembrando que o sistema de equações é dado por:\n",
        "\n",
        "$$\\begin{array}{ccc}\n",
        "a_{11}x_1 + a_{12}x_2 + \\ldots + a_{1n}x_n & = & b_1 \\\\\n",
        "a_{21}x_1 + a_{22}x_2 + \\ldots + a_{2n}x_n & = & b_2\\\\\n",
        "\\vdots & & \\\\\n",
        "a_{n1}x_1 + a_{n2}x_2 + \\ldots + a_{nn}x_n & = & b_n\\\\\n",
        "\\end{array}$$\n",
        "\n",
        "**Método de Jacobi**\n",
        "\n",
        "A partir de uma aproximação inicial ${\\bf x}^{(0)}$ e supondo que $a_{ii}\\neq 0, \\forall i$, o método é obtido isolando cada uma das incógnitas em cada uma das equações. A fórmula de iteração é dada por:\n",
        "\n",
        "$$\\begin{array}{ccc}\n",
        "x_1^{(k+1)} & = & \\dfrac{1}{a_{11}}(b_1 - a_{12}x_2^{(k)} - a_{13}x_3^{(k)} - \\ldots - a_{1n}x_n^{(k)}) \\\\\n",
        " x_2^{(k+1)} & = & \\dfrac{1}{a_{22}}(b_2 - a_{21}x_1^{(k)} - a_{23}x_3^{(k)} - \\ldots - a_{2n}x_n^{(k)})\\\\\n",
        "\\vdots & & \\\\\n",
        "x_n^{(k+1)} & = & \\dfrac{1}{a_{nn}}(b_n - a_{n1}x_1^{(k)} - a_{n2}x_2^{(k)} - \\ldots - a_{nn-1}x_{n-1}^{(k)} \\\\\n",
        "\\end{array}$$\n",
        "\n",
        "\n",
        "Exemplo:\n",
        "\n",
        "$$\\begin{array}{ccc}\n",
        "4x_1 + x_2 + x_3 & = & 5 \\\\\n",
        "-2x_1 + 5x_2 + x_3 & = & 0\\\\\n",
        "3x_1 + x_2 + 6x_3 & = & -6.5\\\\\n",
        "\\end{array}$$"
      ],
      "metadata": {
        "id": "B6cEfE8Dewzr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ik_ezqFdISBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Algoritmo: Método de Jacobi*\n",
        "\n",
        "Dados $A$, ${\\bf b}$ e ${\\bf x}^{(0)}$\n",
        "\n",
        "1. resmax $= 1$\n",
        "\n",
        "2. tol $= 10^{-6}$\n",
        "\n",
        "3. it $= 1$\n",
        "\n",
        "4. itmax $= 1000$\n",
        "\n",
        "5. Enquanto (resmax > tol e it < itmax) faça\n",
        "\n",
        "6. $\\hspace{1pc}$ Para i=1 até n\n",
        "\n",
        "7. $\\hspace{2pc}\\displaystyle{x_i = (b_i - \\sum_{j=1,j\\neq i}^na_{ij}x_j^{(0)})/a_{ii}}$\n",
        "\n",
        "8. $\\hspace{1pc}$ x0 = x\n",
        "   \n",
        "9. $\\hspace{1pc}$ resmax $\\displaystyle{= \\mbox{max}_{1\\le i\\le n} |r_i|}$ com r $= ||b-Ax||$.\n",
        "   \n",
        "10. $\\hspace{1pc}$ it $=$ it $+$ 1\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kxD3Ig-fbgNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Método de Gauss-Seidel**\n",
        "\n",
        "A partir de uma aproximação inicial ${\\bf x}^{(0)}$ e supondo que $a_{ii}\\neq 0, \\forall i$, o método é obtido isolando cada uma das incógnitas em cada uma das equações. A fórmula de iteração é dada por:\n",
        "\n",
        "$$\\begin{array}{rcl}\n",
        "x_1^{(k+1)} & = & \\dfrac{1}{a_{11}}(b_1 - a_{12}x_2^{(k)} - a_{13}x_3^{(k)} - \\ldots - a_{1n}x_n^{(k)}) \\\\\n",
        " x_2^{(k+1)} & = & \\dfrac{1}{a_{22}}(b_2 - a_{21}x_1^{(k+1)} - a_{23}x_3^{(k)} - \\ldots - a_{2n}x_n^{(k)})\\\\\n",
        "\\vdots & & \\\\\n",
        "x_n^{(k+1)} & = & \\dfrac{1}{a_{nn}}(b_n - a_{n1}x_1^{(k+1)} - a_{n2}x_2^{(k+1)} - \\ldots - a_{nn-1}x_{n-1}^{(k+1)}) \\\\\n",
        "\\end{array}$$\n",
        "\n",
        "A diferença entre os dois métodos é que no método de Jacobi utiliza todas as incógnitas da iteração anterior para o cálculo da aproximação na iteração atual. Já no método de Gauss-Seidel, as incógnitas já atualizadas são utilizadas na própria iteração.\n",
        "\n",
        "Exemplo:\n",
        "\n",
        "$$\\begin{array}{ccc}\n",
        "4x_1 + x_2 + x_3 & = & 5 \\\\\n",
        "-2x_1 + 5x_2 + x_3 & = & 0\\\\\n",
        "3x_1 + x_2 + 6x_3 & = & -6.5\\\\\n",
        "\\end{array}$$\n",
        "\n",
        "A fórmula de iteração do método de Gauss-Seidel é dada por:\n",
        "\n",
        " $$\\begin{array}{rcl}\n",
        "x_1^{(k+1)} & = & \\dfrac{1}{4}(5 - x_2^{(k)} - x_3^{(k)}) \\\\\n",
        " x_2^{(k+1)} & = & \\dfrac{1}{5}(0 + 2x_1^{(k+1)} - x_3^{(k)})\\\\\n",
        "x_3^{(k+1)} & = & \\dfrac{1}{6}(-6.5 - 3x_1^{(k+1)} - x_2^{(k+1)})\\\\\n",
        "\\end{array}$$\n",
        "\n",
        "Partindo de $x^{(0)}=(0, 0, 0)$ obtemos:\n",
        "\n"
      ],
      "metadata": {
        "id": "Vy-GQq8YZurS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vjhk8c8tdUeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Algoritmo: Método de Gauss-Seidel*\n",
        "\n",
        "Dados $A$, ${\\bf b}$ e ${\\bf x}^{(0)}$\n",
        "\n",
        "*Algoritmo: Método de Jacobi*\n",
        "\n",
        "Dados $A$, ${\\bf b}$ e ${\\bf x}^{(0)}$\n",
        "\n",
        "1. resmax $= 1$\n",
        "\n",
        "2. tol $= 10^{-6}$\n",
        "\n",
        "3. it $= 1$\n",
        "\n",
        "4. itmax $= 1000$\n",
        "\n",
        "5. Enquanto (resmax > tol e it < itmax) faça\n",
        "\n",
        "6. $\\hspace{1pc}$ Para i=1 até n\n",
        "\n",
        "7. $\\hspace{2pc}\\displaystyle{x_i = (b_i - \\sum_{j=1}^{i-1}a_{ij}x_j - \\sum_{j=i+1}^na_{ij}x_j^{(0)})/a_{ii}}$\n",
        "\n",
        "8. $\\hspace{1pc}$ x0 = x\n",
        "   \n",
        "9. $\\hspace{1pc}$ resmax $\\displaystyle{= \\mbox{max}_{1\\le i\\le n} |r_i|}$ com r $= ||b-Ax||$.\n",
        "   \n",
        "10. $\\hspace{1pc}$ it $=$ it $+$ 1\n"
      ],
      "metadata": {
        "id": "-giki8nzdWOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Critério de parada*:\n",
        "\n",
        "Nos dois algoritmos estamos considerando como critério de parada o valor máximo do resíduo. Ou seja, a repetição é executada enquanto o máximo resíduo for maior que uma tolerância.\n",
        "\n",
        "Outro critério de parada a ser considerado é o máximo da diferença entre as aproximações de duas iterações consecutivas. Ou seja, $\\max_{1\\le i \\le n}|x_i - x_i^{(0)}|$.\n",
        "\n",
        "\n",
        "**Convergência dos métodos de Jacobi e Gauss-Seidel**\n",
        "\n",
        "Considere a decomposição da matriz $A=L+D+U$, sendo:\n",
        "\n",
        "*   $L$ a parte estritamente triangular inferior de $A$, com $l_{ij}=a_{ij}$ se $i>j$ e $l_{ij}=0$ se $i\\le j$;\n",
        "\n",
        "*   $D$ é uma matriz diagonal com $d_{ii}=a_{ii}, \\forall i,$ e;\n",
        "\n",
        "*  $U$ a parte estritamente triangular superior de $A$, com $u_{ij}=0$ se $i\\ge j$ e $u_{ij}=a_{ij}$ se $i<j$.\n",
        "\n",
        "O método de Jacobi, na forma matricial é dado por:\n",
        "\n",
        "$$D{\\bf x}^{(k+1)} = {\\bf b} - (L + U){\\bf x}^{(k)}\\Longrightarrow {\\bf x}^{(k+1)} = D^{-1}({\\bf b} - (L + U){\\bf x}^{(k)})$$\n",
        "\n",
        "O método de Gauss-Seidel, na forma matricial é dado por:\n",
        "\n",
        "$$(L+D){\\bf x}^{(k+1)} = {\\bf b} - U{\\bf x}^{(k)}\\Longrightarrow {\\bf x}^{(k+1)} = (L+D)^{-1}({\\bf b} - (L + U){\\bf x}^{(k)})$$\n",
        "\n",
        "A análise da convergência é feita reescrevendo as fórmulas de iteração no formato: ${\\bf x}^{(k+1)}=B{\\bf x}^k+ {\\bf c}$, onde $B$ é chamada matriz de iteração.\n",
        "\n",
        "A matriz de iteração do método de Jacobi é $B_J=-D^{-1}(L+U)$ e o vetor ${\\bf c} = D^{-1}{\\bf b}$. A matriz de iteração do método de Gauss-Seidel é $B_{GS}=-(L+D)^{-1}U$ e o vetor ${\\bf c} = (L+D)^{-1}{\\bf b}$.\n",
        "\n",
        "Após $k$ iterações obtemos ${\\bf x}^{(k)} = B^k{\\bf x}^{(0)} + \\tilde{{\\bf c}}$. Por exemplo, ${\\bf x}^{(1)} = B{\\bf x}^{(0)} + {\\bf c}$ e ${\\bf x}^{(2)}=B{\\bf x}^{(1)} + {\\bf c}$. Portanto, ${\\bf x}^{(2)}=B(B{\\bf x}^{(0)} + {\\bf c}) + {\\bf c} = B^2{\\bf x}^{(0)}+B{\\bf c}+ {\\bf c},$ com $\\tilde{\\bf c}=B{\\bf c} + {\\bf c}.$"
      ],
      "metadata": {
        "id": "HfuCOXXwgsI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A convergência é estudada por meio da análise dos autovalores da matriz de iteração $B$. Os autovalores $\\lambda$ da matriz $B$ são obtidos a partir de $B{\\bf u} = \\lambda{\\bf u}$, onde ${\\bf u}$ são os autovetores de $B$.\n",
        "\n",
        "**Teorema** O método iterativo ${\\bf x}^{(k)}=B^k{\\bf x}^{(0)}+{\\bf c}$ converge se e somente se os autovalores de $B$ satisfazem $\\max_{1\\le i\\le n}|\\lambda_i|<1.$\n",
        "\n"
      ],
      "metadata": {
        "id": "UpwO2dmK0uQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Critérios de Convergência (condições suficientes):**\n",
        "\n",
        "Para verificarmos na prática: *Critério das Linhas* e *Critério de Sassenfeld*.\n",
        "\n",
        "**Critério das Linhas:** Se a matriz $A$ é estritamente diagonal dominante, então os métodos iterativos convergem. Ou seja, $\\forall i$\n",
        "\n",
        "$$ |a_{ii}| > \\sum_{i=1,j\\neq i} |a_{ij}|$$\n",
        "\n",
        "**Critério de Sassenfeld:** Para avaliar este critério vamos definir os parâmetros $\\beta_i$ associados a cada linha $i$ da matriz de coeficientes $A$. Sejam,\n",
        "\n",
        "$$\\beta_1 = \\frac{\\sum_{j=2}^n|a_{ij}|}{|a_{11}|} \\ \\mbox{ e }\\ \\beta_i=\\frac{\\sum_{j=1}^{i-1}|a_{ij}|\\beta_j+\\sum_{j=i+1}^{n}|a_{ij}|}{|a_{ii}|}, \\mbox{ para } i=2,\\ldots,n.$$\n",
        "\n",
        "Por indução, mostra-se que ${\\bf e}^{(k+1)}\\le \\beta{\\bf e}^{(k)}$, com $\\beta = \\max_{1\\le i\\le n}\\beta_i,$ onde ${\\bf e}{(k)}$ é $k$-ésima aproximação da solução do problema residual $A{\\bf e}={\\bf r}$. Note que,\n",
        "$${\\bf e}^{(k+1)}\\le \\beta{\\bf e}^{(k)}\\le \\beta(\\beta {\\bf e}^{(k-1)}) = \\beta^2{\\bf e}^{(k-1)}.$$\n",
        "\n",
        "Dessa forma,\n",
        "\n",
        "$${\\bf e}^{(k+1)} \\le \\beta^{k+1}{\\bf e}^{(0)},$$\n",
        "sendo ${\\bf e}^{(0)} = {\\bf x}^{(1)} - {\\bf x}^{(0)}$, a diferença entre duas aproximações de iterações consecutivas. Assim,\n",
        "\n",
        "$$\\lim_{k\\rightarrow\\infty}\\|{\\bf e}\\|\\le \\|{\\bf e}^{(0)}\\|\\lim_{k\\rightarrow\\infty}\\beta^{k} = 0,$$\n",
        "\n",
        "somente se $\\beta< 1$.\n",
        "\n",
        "Portanto, se $\\beta<1$ os métodos iterativos convergem. Além disso, quanto menor $\\beta$ mais rápida será a convergência.\n",
        "\n"
      ],
      "metadata": {
        "id": "gEoLu7dOYELW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gzRM-gfVGK1s"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Método SOR** (*Sucessive Over Relaxation*)\n",
        "\n",
        "A *Relaxação Sucessiva* é uma técnica de aceleração de convergência dos métodos iterativos. A aproximação ${\\bf x}^{(k+1)}$ é uma média ponderada entre ${\\bf x}^{(k)} e a aproximação que seria obtida pelo método de Gauss-Seidel. Para cada componente de ${\\bf x}$ temos\n",
        "\n",
        "$$x_i^{(k+1)} = (1-\\omega)x_i^{(k)} + \\frac{\\omega}{a_{ii}}(b_i - \\sum_{j=1}^{i-1}a_{ij}x_j^{(k+1)}-\\sum_{j=i+1}^na_{ij}x_j^{(k)},$$\n",
        "\n",
        "onde $\\omega$ é o parâmetro de relaxação e sua escolha varia de $0<\\omega<2$ (o método só converge nesse intervalo). Se $\\omega=1$ temos o método de Gauss-Seidel. Para alguns problemas é possível determinar $\\omega$ ótimo.\n",
        "\n",
        "*Exemplo:*\n",
        "\n",
        "Vamos aplicar o Método SOR para resolver:\n",
        "\n",
        "$$\\begin{array}{ccccccccc}\n",
        "2x_1 & - & x_2 &  &  &  & & = & 1\\\\\n",
        "-x_1 & + & 2x_2 & - & x_3 & & & = & 0\\\\\n",
        "& & \\ddots & \\\\\n",
        "& - & x_{n-2} & + & 2x_{n-1} & - & x_n & = & 0\\\\\n",
        "& & & - & x_{n-1} & + & 2x_n & = & 1\n",
        "\\end{array}$$\n",
        "\n"
      ],
      "metadata": {
        "id": "efTwVbQGYlvv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KHAloJ2VKVfZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}